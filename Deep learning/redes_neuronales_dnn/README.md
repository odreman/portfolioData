# Redes Neuronales DNN

## üìã Descripci√≥n

Dos proyectos que demuestran el uso de redes neuronales densas (DNN) para resolver problemas de clasificaci√≥n. El primero resuelve el problema cl√°sico XOR (no linealmente separable) y el segundo clasifica im√°genes de ropa del dataset Fashion MNIST.

## üéØ Objetivos

1. **Problema XOR:** Demostrar que las redes neuronales multicapa pueden resolver problemas no linealmente separables (ser m√°s listos que los SVMs)
2. **Fashion MNIST:** Clasificar im√°genes de prendas de ropa en 10 categor√≠as diferentes

### Objetivos de Aprendizaje

- Conocer y aplicar redes neuronales profundas en problemas supervisados de machine learning
- Familiarizarse con TensorFlow y Keras como frameworks para desarrollar redes neuronales
- Comprender la normalizaci√≥n en Deep Learning
- Comprender la tasa de aprendizaje (learning rate) en Deep Learning
- Aprender a construir un problema y un modelo desde cero

## üìö Notebooks

### 1. Problema XOR (`S5_DL_practice1_XOR.ipynb`)
Implementaci√≥n de una DNN para resolver el problema XOR, demostrando la capacidad de las redes multicapa para aprender funciones no lineales.

**Caracter√≠sticas:**
- Arquitectura: 2-2-1 (2 entradas, 2 neuronas ocultas, 1 salida)
- Funci√≥n de activaci√≥n: Sigmoid en capa oculta
- Optimizador: SGD con learning rate 0.1
- Entrenamiento: 1000 √©pocas

**Resultados:**
- La red aprende correctamente la funci√≥n XOR
- Predicciones obtenidas:
  - [0,0] ‚Üí 0.3034 (esperado: 0)
  - [0,1] ‚Üí 0.5404 (esperado: 1)
  - [1,0] ‚Üí 0.5362 (esperado: 1)
  - [1,1] ‚Üí 0.6198 (esperado: 0)

**An√°lisis de Pesos:**
- La capa oculta transforma el espacio 2D de entrada en un nuevo espacio 2D
- La funci√≥n sigmoid "dobla" el espacio para separar los puntos del XOR
- La capa de salida combina las activaciones de la capa oculta para formar las regiones de decisi√≥n
- El XOR funciona porque la capa oculta crea dos l√≠neas de decisi√≥n que, al combinarse, forman las regiones necesarias para separar los puntos (0,0),(1,1) de los puntos (0,1),(1,0)

### 2. Fashion MNIST (`S5_DL_practice1_FasMNIST.ipynb`)
Clasificaci√≥n de im√°genes de ropa en 10 categor√≠as usando una DNN optimizada.

**Caracter√≠sticas:**
- 10 clases: Camisetas, pantalones, zapatos, etc.
- Arquitectura: M√∫ltiples capas densas
- Optimizaci√≥n de hiperpar√°metros y t√©cnicas de normalizaci√≥n

**T√©cnicas Implementadas:**

1. **Normalizaci√≥n de Datos (TODOs 1-3):**
   - Escalado a rango [0,1]: Dividir por 255.0
   - Centrado a media 0: Restar la media
   - Estandarizaci√≥n N(0,1): Normalizar a media 0 y varianza 1

2. **An√°lisis de Normalizaci√≥n (TODO 4):**
   - Comparaci√≥n de 3 tipos de normalizaci√≥n
   - **Mejor resultado:** Estandarizaci√≥n N(0,1) con 88.71% de precisi√≥n
   - La normalizaci√≥n mejora significativamente la convergencia y estabilidad

3. **Optimizaci√≥n de Learning Rate (TODO 5):**
   - Experimentaci√≥n con learning rates: [0.1, 0.001, 0.0001]
   - **Mejor resultado:** 0.001 con 88.25% de precisi√≥n
   - Learning rate alto (0.1) impide el aprendizaje
   - Learning rate bajo (0.0001) ralentiza innecesariamente

4. **Callbacks (TODOs 6-7):**
   - Implementaci√≥n de ReduceLROnPlateau
   - **Mejora significativa:** De 88.25% a 90.14% de precisi√≥n
   - Ajuste autom√°tico del learning rate cuando la p√©rdida se estanca

## üîß Tecnolog√≠as Utilizadas

- **TensorFlow/Keras:** Framework principal
- **DNN:** Redes neuronales densas (fully connected)
- **NumPy:** Procesamiento num√©rico
- **Matplotlib:** Visualizaci√≥n

## üìä Datasets

### XOR
- 4 ejemplos de entrenamiento
- Entrada binaria (2D)
- Salida binaria

### Fashion MNIST
- 70,000 im√°genes (28x28 p√≠xeles)
- 10 clases de prendas de ropa
- Divisi√≥n: 60,000 train / 10,000 test

## üèóÔ∏è Arquitecturas

### XOR
```
Input (2) ‚Üí Hidden (2) ‚Üí Output (1)
```

### Fashion MNIST
```
Input (784) ‚Üí Hidden Layers ‚Üí Output (10)
```

## üìà Caracter√≠sticas Principales

### Fashion MNIST

- **Normalizaci√≥n exhaustiva:** Comparaci√≥n de 3 tipos (rango [0,1], media 0, N(0,1))
- **Optimizaci√≥n de learning rate:** Experimentaci√≥n sistem√°tica con diferentes valores
- **Callbacks avanzados:** ReduceLROnPlateau para ajuste autom√°tico del learning rate
- **An√°lisis de curvas de aprendizaje:** Evaluaci√≥n detallada de convergencia y estabilidad

### Problema XOR

- **Resoluci√≥n de problemas no linealmente separables:** Demostraci√≥n pr√°ctica de la necesidad de capas ocultas
- **An√°lisis de pesos entrenados:** Comprensi√≥n de c√≥mo la red transforma el espacio de entrada
- **Visualizaci√≥n de la transformaci√≥n:** Explicaci√≥n de c√≥mo las capas ocultas crean regiones de decisi√≥n

## üéØ Resultados Clave

### Fashion MNIST

| T√©cnica | Configuraci√≥n | Test Accuracy |
|---------|---------------|---------------|
| **ReduceLROnPlateau** | Callback con patience=2, factor=0.5 | **0.9014** ‚≠ê |
| Learning Rate √≥ptimo | 0.001 (fijo) | 0.8825 |
| Normalizaci√≥n √≥ptima | Estandarizaci√≥n N(0,1) | 0.8871 |
| Normalizaci√≥n b√°sica | Rango [0,1] | 0.8732 |
| Centrado | Media 0 | 0.8768 |

**Hallazgos:**
- La estandarizaci√≥n N(0,1) obtiene el mejor resultado entre las normalizaciones (88.71%)
- El learning rate de 0.001 ofrece el mejor equilibrio entre velocidad y precisi√≥n
- ReduceLROnPlateau mejora significativamente el resultado final (90.14%)
- Las normalizaciones mejoran la convergencia y estabilidad del entrenamiento

### Problema XOR

- **√âxito en resoluci√≥n:** La red aprende correctamente la funci√≥n XOR
- **Arquitectura m√≠nima:** Solo 2 neuronas ocultas son suficientes
- **Comprensi√≥n de pesos:** An√°lisis detallado de c√≥mo la red transforma el espacio de entrada

## üöÄ Ejecuci√≥n

```bash
# Instalar dependencias
pip install tensorflow numpy matplotlib

# Ejecutar notebooks
cd notebooks
jupyter notebook S5_DL_practice1_XOR.ipynb
jupyter notebook S5_DL_practice1_FasMNIST.ipynb
```

## üìù Notas

### Problema XOR
- Es un ejemplo cl√°sico que demuestra la necesidad de capas ocultas
- Un perceptr√≥n simple no puede resolver XOR (problema no linealmente separable)
- Las redes multicapa pueden aprender funciones no lineales mediante transformaciones del espacio

### Fashion MNIST
- M√°s complejo que MNIST tradicional (d√≠gitos)
- Requiere t√©cnicas de normalizaci√≥n para obtener buenos resultados
- Los callbacks son fundamentales para optimizar el entrenamiento
- La normalizaci√≥n N(0,1) es crucial para la convergencia estable

### T√©cnicas Aprendidas
- Normalizaci√≥n de datos: Escalado, centrado y estandarizaci√≥n
- Optimizaci√≥n de learning rate: Balance entre velocidad y estabilidad
- Callbacks: Ajuste autom√°tico de hiperpar√°metros durante el entrenamiento
- An√°lisis de arquitecturas: Comprensi√≥n de c√≥mo las capas transforman los datos

