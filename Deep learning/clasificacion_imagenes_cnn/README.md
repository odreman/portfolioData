# Clasificaci√≥n de Im√°genes con CNN

## üìã Descripci√≥n

Serie de proyectos que progresan desde la construcci√≥n de una CNN b√°sica hasta t√©cnicas avanzadas de optimizaci√≥n para clasificaci√≥n de im√°genes (perros vs gatos). Cada notebook aborda diferentes aspectos de la construcci√≥n y optimizaci√≥n de redes neuronales convolucionales.

## üéØ Objetivos

1. **CNN desde cero:** Construir una red convolucional b√°sica
2. **Reducci√≥n de sobreentrenamiento:** Implementar data augmentation y dropout
3. **Transfer learning:** Utilizar modelos pre-entrenados
4. **Optimizaci√≥n avanzada:** Fine-tuning y t√©cnicas avanzadas

## üìö Notebooks

### 1. CNN desde cero (`01_cnn_template.ipynb`)
Construcci√≥n de una CNN b√°sica para clasificaci√≥n binaria de im√°genes.

### 2. Reducci√≥n de sobreentrenamiento (`02_cnn_template.ipynb`)
Implementaci√≥n de data augmentation y dropout para mejorar la generalizaci√≥n.

### 3. Transfer Learning (`03_cnn_template.ipynb`)
Uso de modelos pre-entrenados (VGG, ResNet, Inception V3, etc.) para mejorar el rendimiento. Incluye explicaci√≥n detallada de la arquitectura Inception V3 y su aplicaci√≥n a diferentes dominios.

### 4. Optimizaci√≥n avanzada (`04_cnn_template_.ipynb`)
Aplicaci√≥n de Inception V3 a clasificaci√≥n de flores, demostrando la versatilidad del transfer learning. Comparaci√≥n de resultados con y sin Inception V3.

## üîß Tecnolog√≠as Utilizadas

- **TensorFlow/Keras:** Framework principal
- **CNN:** Redes neuronales convolucionales
- **Transfer Learning:** Modelos pre-entrenados (VGG, ResNet, etc.)
- **Data Augmentation:** Transformaciones de im√°genes
- **OpenCV/PIL:** Procesamiento de im√°genes

## üìä Datasets

### Dogs vs Cats
- **Dataset:** 2,000 im√°genes de perros y gatos
- **Divisi√≥n:** Train/Validation
- **Preprocesamiento:** Redimensionamiento a 150x150, normalizaci√≥n
- **Uso:** Notebooks 1-3

### Clasificaci√≥n de Flores
- **Dataset:** Base de datos de flores (m√∫ltiples clases)
- **Divisi√≥n:** Train/Validation
- **Preprocesamiento:** Redimensionamiento, normalizaci√≥n
- **Uso:** Notebook 4 (comparaci√≥n CNN propia vs Inception V3)

## üèóÔ∏è Arquitecturas

- CNN personalizada (capas convolucionales, pooling, dense)
- Modelos pre-entrenados (VGG16, ResNet50, etc.)
- Fine-tuning de capas finales

## üìà T√©cnicas Implementadas

### Data Augmentation

El proyecto implementa data augmentation exhaustivo para mejorar la generalizaci√≥n del modelo. Las transformaciones aplicadas incluyen:

- **rotation_range=40:** Rotaci√≥n aleatoria de ¬±40 grados para reconocer objetos en diferentes orientaciones
- **width_shift_range=0.2:** Desplazamiento horizontal hasta 20% del ancho para simular cambios de posici√≥n lateral
- **height_shift_range=0.2:** Desplazamiento vertical hasta 20% de la altura para generalizar ante cambios de posici√≥n vertical
- **shear_range=0.2:** Transformaci√≥n de cizalladura diagonal para robustez ante distorsiones
- **zoom_range=0.2:** Zoom aleatorio de ¬±20% para reconocer objetos a diferentes escalas
- **horizontal_flip=True:** Volteo horizontal aleatorio (im√°genes en espejo)
- **fill_mode='nearest':** Relleno de espacios vac√≠os copiando el p√≠xel m√°s cercano

Estas transformaciones aseguran que el modelo nunca vea la misma imagen dos veces durante el entrenamiento, aumentando la variabilidad y fortaleciendo la capacidad de generalizaci√≥n.

### Dropout - Experimentaci√≥n Sistem√°tica

Se realiz√≥ una experimentaci√≥n exhaustiva con diferentes valores de dropout para encontrar el equilibrio √≥ptimo:

| Dropout | Train Accuracy | Val Accuracy | Train Loss | Val Loss | Diferencia Train-Val |
|---------|----------------|--------------|------------|----------|----------------------|
| **0.5** | 0.7480 | **0.7840** ‚≠ê | 0.5069 | **0.4771** | 0.0360 |
| 0.3 | 0.7590 | 0.7630 | 0.5085 | 0.4791 | 0.0040 |
| 0.7 | 0.7550 | 0.7610 | 0.5114 | 0.5101 | 0.0060 |
| 0.1 | 0.7455 | 0.7730 | 0.5144 | 0.4807 | 0.0275 |
| 0.9 | 0.7010 | 0.7520 | 0.5849 | 0.5257 | 0.0510 |

**Hallazgos:**
- **Dropout 0.5 fue seleccionado** como el valor √≥ptimo, ofreciendo el mejor equilibrio entre evitar sobreajuste y mantener capacidad de aprendizaje
- Valores intermedios (0.3-0.7) ofrecen mejor equilibrio que extremos
- Dropout muy alto (0.9) limita demasiado el aprendizaje (underfitting)
- Dropout muy bajo (0.1) puede llevar a sobreajuste, aunque en este caso la diferencia fue peque√±a

### Transfer Learning con Inception V3

El proyecto implementa transfer learning utilizando **Inception V3**, una arquitectura de red convolucional profunda.

**Caracter√≠sticas de Inception V3:**
- **M√≥dulos Inception:** Utiliza m√∫ltiples tama√±os de filtros (1x1, 3x3, 5x5) en el mismo bloque para capturar patrones a diferentes escalas
- **T√©cnicas de optimizaci√≥n:**
  - Convoluciones 1x1 para reducir dimensiones
  - Factorizaci√≥n de convoluciones (5x5 ‚Üí dos 3x3)
  - Average Pooling y Batch Normalization
  - Auxiliary classifiers para mejorar flujo de gradientes
- **Entrenamiento:** Pre-entrenado en ImageNet (1+ mill√≥n de im√°genes, 1000 clases) usando GPUs, data augmentation y SGD con regularizaci√≥n

**Aplicaci√≥n a diferentes dominios:**
- Aunque entrenado en im√°genes de 299x299, funciona con 150x150 gracias a que las convoluciones trabajan por zonas locales
- Se aplic√≥ exitosamente a clasificaci√≥n de flores, demostrando su versatilidad

**Resultados con Inception V3 (clasificaci√≥n de flores):**
- **Sin Inception V3:** Accuracy de validaci√≥n ~0.70 (70%) con mayor variabilidad e inestabilidad
- **Con Inception V3:** Accuracy de validaci√≥n 0.74-0.76 (74-76%) con mayor estabilidad y menos overfitting
- **Conclusi√≥n:** Inception V3 proporciona un salto claro de rendimiento incluso con pocos datos, gracias a los pesos pre-entrenados y la arquitectura profunda

### Otras T√©cnicas

- Transfer learning con otros modelos (VGG16, ResNet50)
- Fine-tuning de capas finales
- Early stopping
- Callbacks de Keras

## üöÄ Ejecuci√≥n

```bash
# Instalar dependencias
pip install tensorflow opencv-python pillow numpy matplotlib

# Ejecutar notebooks
jupyter notebook 01_cnn_template.ipynb
```

## üéØ Resultados Clave

### Regularizaci√≥n (Notebook 2)
- **Mejor dropout:** 0.5 (val accuracy: 0.7840)
- Data augmentation reduce significativamente el sobreajuste
- Valores intermedios de dropout (0.3-0.7) ofrecen mejor equilibrio

### Transfer Learning (Notebooks 3-4)
- **Inception V3** mejora significativamente el rendimiento
- Aplicable a diferentes dominios (perros/gatos ‚Üí flores)
- Funciona con diferentes tama√±os de imagen (299x299 ‚Üí 150x150)
- Mejora de ~70% a ~74-76% en clasificaci√≥n de flores

## üìù Notas

- Se recomienda usar GPU para entrenamiento (especialmente con Inception V3)
- Los datos se descargan autom√°ticamente
- Los modelos pueden guardarse para reutilizaci√≥n
- Inception V3 requiere descarga de pesos pre-entrenados (autom√°tico con Keras)

