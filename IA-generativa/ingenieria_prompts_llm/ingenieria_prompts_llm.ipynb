{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zzXwuNu4fGN"
   },
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuSLtqMF4wbN"
   },
   "source": [
    "## ANEXO - Traductor automático\n\nA lo largo de este ejercicio se hacer uso de un modelo LLM entrenado principalmente con texto en inglés, por tanto, se debe interactuar con él en el mismo idioma.\n\nOs dejo a continuación una pequeña ayuda para que podáis utilizar un traductor automático en caso de que se quiera interactuar con el modelo en castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DBOLhJEJ3sgy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==3.1.0a0 in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.6.15)\n",
      "Requirement already satisfied: hstspreload in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
      "Requirement already satisfied: chardet==3.* in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==3.1.0a0\n",
    "\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def traducir(text):\n",
    "  return translator.translate(text, dest='en', src='es').text\n",
    "\n",
    "def translate(text):\n",
    "  return translator.translate(text, dest='es', src='en').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lyC5VwF5Dqe"
   },
   "source": [
    "Lo único que deberéis hacer es utilizar las funciones *traducir('texto')* y *translate('text')* para pasar del castellano al inglés y del inglés al castellano respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRs65TMC3yHc"
   },
   "source": [
    "# Nuestro primer LLM: Resumen de Diálogos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OIBTsjx4KMq"
   },
   "source": [
    "## 1 - Configurando el entorno\n",
    "\n",
    "Este ejercicio hace uso de redes relativamente pesadas por lo que recomiendo que os aseguréis de que colab está usando un kernel apropiado (GPU o TPU)\n",
    "\n",
    "A continuación, vampos a proceder a instalar los paquetes y datasets necesarios.\n",
    "\n",
    "La siguiente celda puede tardar varios minutos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GgSlAuyx4BGE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/envs/iagen/lib/python3.9/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==2.6.0 \\\n",
    "    torchdata==0.11.0 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.41.0 \\\n",
    "    datasets==2.10.0  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hImh38UIkOK1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \\\n",
    "    transformers==4.41.0 \\\n",
    "    datasets==2.10.0  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEBsKre5j4Be"
   },
   "source": [
    "Actualizamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3C-3gsppj-2p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AHlQoGbL4enF"
   },
   "outputs": [],
   "source": [
    "#Cargamos los datasets, el modelo LLM, el tokenizador y el configurador. No te preocupes si aún no entiendes estos componentes, los vamos a desgranar cuando los utilicemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SKvf_9Ma4E37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoWqC1CD4ij1"
   },
   "source": [
    "<a name='2'></a>\n\n## 2 - Resumen de diálogos sin Prompt Engineering\n\nPara este ejercicio se generar resúmenes de diálogos con el modelo LLM pre-entrenado FLAN-T5 (https://www.datacamp.com/tutorial/flan-t5-tutorial) que descargaremos de Hugging Face (https://huggingface.co/docs/transformers/model_doc/flan-t5). La lista de modelos transformer disponibles puede encontrarse [aquí](https://huggingface.co/docs/transformers/index).\n\nse utilizar algunos diálogos sencillos de la base de datos [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) También disponible en Hugging Face. Este dataset contiene más de 10,000 diálogos con sus etiquetas correspondientes (resúmenes y temática)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 4.65kB [00:00, 4.51MB/s]\n",
      "Downloading data: 100%|██████████| 11.3M/11.3M [00:00<00:00, 46.1MB/s]\n",
      "Downloading data: 100%|██████████| 11.3M/11.3M [00:00<00:00, 46.1MB/s]\n",
      "Downloading data: 100%|██████████| 442k/442k [00:00<00:00, 3.10MB/s]]\n",
      "Downloading data: 100%|██████████| 442k/442k [00:00<00:00, 1.93MB/s]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:00<00:00, 8.68MB/s]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:00<00:00, 8.56MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 820.59it/s]\n",
      "Generating train split:   0%|          | 0/24920 [00:00<?, ? examples/s]/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:751: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:751: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating train split: 100%|██████████| 24920/24920 [00:00<00:00, 118265.31 examples/s]\n",
      "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:751: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:751: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating validation split: 100%|██████████| 1000/1000 [00:00<00:00, 83884.40 examples/s]\n",
      "Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:751: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:751: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating test split: 100%|██████████| 3000/3000 [00:00<00:00, 148008.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"knkarthick/dialogsum\", download_mode=\"force_redownload\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbaJFOi84qbx"
   },
   "source": [
    "Como siempre, se trabajar un poquitín con los datos, aquí se dejar un poquitín de código para ver algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_Hr8uUCo4tGG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht8c7BY7BSDC"
   },
   "source": [
    "se cargar nuestro modelo, el FLAN - T5, para ello se crear una instancia de AutoModelForSeq2SeqLM con el método .from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dPAdtWx_BT-L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/iagen/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr2h4q2bBXUN"
   },
   "source": [
    "Para realizar la codificación y decodificación, necesitamos tokenizar el texto. Es decir, dividiremos los textos en unidades más pequeñas que puedan ser procesadas por modelos LLM.\n\nA continuación se descargar el tokenizador para el modelo FLAN-T5 usando el método AutoTokenizer.from_pretrained(). El parámetro use_fast activa el tokenizador rápido. No es necesario entrar en detalles al respecto, pero si tenéis curiosidad se puede echar un ojo a la documentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qWwLCtTABaGn"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjJrRAIkBcya"
   },
   "source": [
    "se probar la codificación y decodificación de las frases tokenizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VFfSSTLUBe68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7JqwpFyBi7L"
   },
   "source": [
    "se explorar qué tal funciona este LLM para resumir un diálogo sin hacer uso de Prompt Engineering. Recordad que Prompt Engineering es modificar el prompt (entrada de los datos) intentando mejorar la respuesta del modelo para una tarea determinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8Ns2qLKXBmAm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWj5R63ABvL1"
   },
   "source": [
    "Como puedes ver las salidas del modelo tienen cierto sentido, pero no parece estar seguro de qué tarea se supone que debe realizar. Parece que simplemente constituye la siguiente oración del diálogo. Metemos un poquito de Prompt Engineering a ver qué pasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSOHbAulB0zy"
   },
   "source": [
    "## 3 - Resumen de Diálogos con Instruction Prompt\n",
    "\n",
    "Prompt engineering es una herramienta muy importante en el uso de modelos básicos para la generación de texto. Os dejo un [link](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) si queréis refrescar conocimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve7bxaR7B4JP"
   },
   "source": [
    "<a name='3.1'></a>\n### 3.1 - Inferencia Zero Shot con Instruction Prompt\n\nPara pedirle a un modelo que realice una tarea - resumir un diálogo en este caso - necesitas coger dicho diálogo y convertirlo en un Instruction Prompt. A esto lo conocemos como **zero shot inference** ya que no utilizamos ningún ejemplo y le pedimos la tarea de forma directa.\n\nse por ello (si tienes dificultad para entender el código, repasa f-string formatting en Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KCdMwMD0B7fi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Provide a one-line summary of the following conversation.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary: ?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Provide a one-line summary of the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary: ?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Provide a one-line summary of the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: ?\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvdcRMcXCEJQ"
   },
   "source": [
    "Bastante mejor, no? Sin embargo, el modelo todavía no hace un resumen que realmente sintetice la información principal... se ver si podemos mejorar esto, os toca!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFJ3gGHwCErQ"
   },
   "source": [
    "**Ejercicio:**\n\n- Experimenta con el texto de `prompt` para ver cómo cambia la inferencia que podemos obtener del modelo. Cambia mucho el resultado cuando terminamos el diálogo sin ninguna especificación vs terminarlo con `Summary: ` Analiza los resultados y detalla si has realizado alguna otra prueba cuyo resultado te haya llamado la atención.\n- Intenta cambiar el principio del `prompt`, reemplaza `Summarize the following conversation.` por algo diferente y comprueba cómo cambia la inferencia del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pyp4r0IVC-Be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EJERCICIO 1: EXPERIMENTACIÓN CON PROMPTS\n",
      "================================================================================\n",
      "\n",
      "1. COMPARACIÓN: Sin especificación vs 'Summary:'\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "EJEMPLO 1:\n",
      "RESUMEN HUMANO: #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "SIN ESPECIFICACIÓN: Tom is late for the train.\n",
      "CON 'Summary:': Tom is late for the train.\n",
      "--------------------------------------------------\n",
      "\n",
      "EJEMPLO 2:\n",
      "RESUMEN HUMANO: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "SIN ESPECIFICACIÓN: #Person1: I'm thinking of upgrading my computer. #Person2: I'm not sure what exactly I would need. #Person1: I'd like to make up my own flyers and banners.\n",
      "CON 'Summary:': #Person1#: I'm thinking of upgrading my computer.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# EJERCICIO 1: EXPERIMENTACIÓN CON PROMPTS\n",
    "print(\"=\"*80)\n",
    "print(\"EJERCICIO 1: EXPERIMENTACIÓN CON PROMPTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Experimento 1: Sin especificación vs con \"Summary:\"\n",
    "print(\"\\n1. COMPARACIÓN: Sin especificación vs 'Summary:'\")\n",
    "print(dash_line)\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    # Prompt sin especificación final\n",
    "    prompt_sin_spec = f\"\"\"\n",
    "Provide a one-line summary of the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt con \"Summary:\" al final\n",
    "    prompt_con_summary = f\"\"\"\n",
    "Provide a one-line summary of the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nEJEMPLO {i + 1}:\")\n",
    "    print(f\"RESUMEN HUMANO: {summary}\")\n",
    "    \n",
    "    # Genero respuestas\n",
    "    inputs1 = tokenizer(prompt_sin_spec, return_tensors='pt')\n",
    "    output1 = tokenizer.decode(\n",
    "        model.generate(inputs1[\"input_ids\"], max_new_tokens=50)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    inputs2 = tokenizer(prompt_con_summary, return_tensors='pt')\n",
    "    output2 = tokenizer.decode(\n",
    "        model.generate(inputs2[\"input_ids\"], max_new_tokens=50)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"SIN ESPECIFICACIÓN: {output1}\")\n",
    "    print(f\"CON 'Summary:': {output2}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. DIFERENTES INICIOS DE PROMPT\n",
      "---------------------------------------------------------------------------------------------------\n",
      "RESUMEN HUMANO: #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "VARIACIÓN 1 - 'Summarize the following conversation.':\n",
      "RESULTADO: The train is about to leave.\n",
      "\n",
      "VARIACIÓN 2 - 'What is the main point of this dialogue?':\n",
      "RESULTADO: Tom is late for the train. He has to catch the nine-thirty train.\n",
      "\n",
      "VARIACIÓN 3 - 'Briefly explain what happened in this conversation.':\n",
      "RESULTADO: Tom is late. He has to catch the nine-thirty train.\n",
      "\n",
      "VARIACIÓN 4 - 'Create a short summary of this discussion.':\n",
      "RESULTADO: Tom is late for work.\n",
      "\n",
      "VARIACIÓN 5 - 'Tell me what this conversation is about in one sentence.':\n",
      "RESULTADO: The train is about to leave.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experimento 2: Diferentes inicios de prompt\n",
    "print(\"\\n\\n2. DIFERENTES INICIOS DE PROMPT\")\n",
    "print(dash_line)\n",
    "\n",
    "prompt_variations = [\n",
    "    \"Summarize the following conversation.\",\n",
    "    \"What is the main point of this dialogue?\", \n",
    "    \"Briefly explain what happened in this conversation.\",\n",
    "    \"Create a short summary of this discussion.\",\n",
    "    \"Tell me what this conversation is about in one sentence.\"\n",
    "]\n",
    "\n",
    "dialogue = dataset['test'][example_indices[0]]['dialogue']\n",
    "summary = dataset['test'][example_indices[0]]['summary']\n",
    "\n",
    "print(f\"RESUMEN HUMANO: {summary}\\n\")\n",
    "\n",
    "for j, variation in enumerate(prompt_variations):\n",
    "    prompt = f\"\"\"\n",
    "{variation}\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"VARIACIÓN {j+1} - '{variation}':\")\n",
    "    print(f\"RESULTADO: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nANÁLISIS EJERCICIO 1: EXPERIMENTACIÓN CON PROMPTS\\n\\n    - Hallazgo principal: Añadir \"Summary:\" no mejora la calidad del resumen - ambas variantes producen resultados \\n    idénticos con el mismo error sistemático de confundir personajes (Tom vs #Person1#).\\n\\n    - Impacto de diferentes inicios: Las preguntas directas (\"What is the main point?\") generan respuestas más elaboradas \\n    que las instrucciones imperativas, pero todos los prompts fallan en mantener roles correctos del diálogo.\\n\\n    - Implicación técnica: FLAN-T5-base presenta limitaciones en zero-shot para comprensión de diálogos multi-personaje, \\n    sugiriendo que el problema es de capacidad del modelo más que de ingeniería de prompt superficial.\\n\\n    - Insight clave: La consistencia en los errores independientemente del formato indica que necesitamos estrategias más \\n    sofisticadas como few-shot learning para guiar al modelo hacia una mejor comprensión contextual.\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    - TODO 1: ¿Cambia mucho el resultado sin especificación vs terminándolo con Summary:?\n",
    "        - No cambia significativamente. \n",
    "            - Ejemplo 1: ambos producen \"Tom is late for the train\" (idénticos). \n",
    "            - Ejemplo 2: sin especificación genera diálogo extendido, con \"Summary:\" produce \n",
    "            solo \"#Person1#: I'm thinking of upgrading my computer\" (más conciso pero ambos incorrectos).\n",
    "\n",
    "    - TODO 2: Cambiar el principio del prompt - ¿cómo cambia la inferencia?\n",
    "        - Diferentes inicios producen variaciones notables: \n",
    "            - \"Summarize\"                   → \"The train is about to leave\" (urgencia), \n",
    "            - \"What is the main point?\"     → respuesta más elaborada con detalles, \n",
    "            - \"Briefly explain\"             → versión condensada, \n",
    "            - \"Create summary\"              → añade información inexistente (\"work\"). \n",
    "            - Las preguntas directas generan respuestas más detalladas que las instrucciones imperativas.\n",
    "\n",
    "    - Observación destacada: Todos los prompts mantienen el error sistemático de confundir personajes (Tom vs #Person1#), \n",
    "    sugiriendo limitaciones fundamentales del modelo que el prompt engineering superficial no puede resolver.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxPI7pXdDBTT"
   },
   "source": [
    "<a name='3.2'></a>\n### 3.2 - Inferencia Zero Shot usando los templates de Prompt de FLAN-T5\n\nse usar un prompt ligeramente diferente. FLAN-T5 tiene muchos templates públicos para ciertas tareas [link](https://github.com/google-research/FLAN/tree/main/flan/v2). A continuación se usar uno de estos [prompts pre-built para FLAN-T5](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "miJXjbutDHOW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AcW87t4DLzw"
   },
   "source": [
    "Como podemos ver este prompt obtiene mejores resultados pero le sigue costando extraer la esencia de las conversaciones... se ver si podemos mejorarlo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kunqMuObDN-x"
   },
   "source": [
    "<a name='4'></a>\n## 4 - Resumen de Diálogos con One Shot y Few Shot\n\n**One shot y few shot inference** consiste en darle al LLM uno o más ejemplos completos de pares prompt-response que se ajustan a tu tarea. A esto se le llama \"in-context learning\" y tiene como objetivo poner al modelo en un estado que facilite la resolución de tu tarea sin necesidad de ajustar pesos. se puede darle un pequeño repaso [en este blog de HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwx77iT_DQL6"
   },
   "source": [
    "<a name='4.1'></a>\n### 4.1 - One Shot Inference\n\nse crear una función que genere un prompt con ejemplos completos a partir de una lista de `example_indices_full` y añade al final el prompt que queremos que el modelo complete, `example_index_to_summarize`. se utilizar el mismo modelo FLAN-T5 de la sección [3.2](#3.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ExQSjC2TDS7b"
   },
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFNpoEyPDZUi"
   },
   "source": [
    "Realizamos el prompt para hacer inferencias one shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "B0tndgykDZ1S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rOnyAiFDdbM"
   },
   "source": [
    "Ahora le pasamos este prompt para realizar la inferencia one shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NP2LnKQrDfkV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qZ2kQk0Dii3"
   },
   "source": [
    "### 4.2 - Few Shot Inference\n\nse explorar few shot inference, es decir, la utilización de más de un ejemplo. Comenzaremos añadiendo dos ejemplos completos al prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "N0Mg-OwrDkj7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tpFr_mgDwno"
   },
   "source": [
    "Ahora podemos pasarle este prompt al modelo para ver los resultados con few shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KNSeXj9uD2_c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Jack tells #Person1# that business communication is his favorite last year and #Person1# will check it.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwpTAcLGD7VR"
   },
   "source": [
    "En este caso podemos ver que few shot no ha conseguido resultados mucho mejores que los que vimos con one shot. Además (y se puede probarlo) añadir más ejemplos no mejora demasiado en este ejemplo y rara vez se utilizan más de 5 o 6 ejemplos en ningún caso ya que no suele mejorar el resultado. Además, tenemos que estar seguros de que no se superar el contexto máximo de nuestro modelo, en este caso, 512 tokens.\n\nSin embargo, hemos podido ver que añadir uno ejemplo complto (one shot) si ha conseguido un modelo con más información y hemos mejorado de forma cualitativa el resumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnlAgnfXD_Rj"
   },
   "source": [
    "**Ejercicio:**\n\nAhora te toca experimentar con few shot inferencing.\n- Elige diferentes diálogos - Cambia los índices en la lista `example_indices_full` y el valor `example_index_to_summarize`. Analiza los resultados.\n- Cambia el número de ejemplos (shots). Asegúrate de no sobrepasar el contexto máximo del modelo (512).\n\nQué puedes observar Cómo de bien funciona few shot inferencing para otros ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FTqoHnljEGuj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EJERCICIO 2: EXPERIMENTACIÓN CON FEW SHOT\n",
      "================================================================================\n",
      "1. DIFERENTES COMBINACIONES DE DIÁLOGOS\n",
      "---------------------------------------------------------------------------------------------------\n",
      "CONFIG 1 - Ejemplos: [40], Target: 200\n",
      "REAL: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "GENERADO: #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n",
      "CONFIG 2 - Ejemplos: [80], Target: 200\n",
      "REAL: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "GENERADO: #Person1 wants to upgrade his system and hardware.\n",
      "\n",
      "CONFIG 3 - Ejemplos: [120], Target: 200\n",
      "REAL: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "GENERADO: #Person1 wants to upgrade his computer. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n",
      "CONFIG 4 - Ejemplos: [40], Target: 180\n",
      "REAL: Jack tells #Person1# that business communication is his favorite last year and #Person1# will check it.\n",
      "GENERADO: Jack's classes were not bad.\n",
      "\n",
      "CONFIG 5 - Ejemplos: [80], Target: 180\n",
      "REAL: Jack tells #Person1# that business communication is his favorite last year and #Person1# will check it.\n",
      "GENERADO: Jack's classes were not bad.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "2. VARIACIÓN EN NÚMERO DE SHOTS\n",
      "---------------------------------------------------------------------------------------------------\n",
      "TARGET: 200 - RESUMEN REAL: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "1-SHOT (tokens: 392): #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "2-SHOT: EXCEDE LÍMITE DE 512 TOKENS\n",
      "   Prompt: 631 + Generación: 50 = 681 tokens total\n",
      "3-SHOT: EXCEDE LÍMITE DE 512 TOKENS\n",
      "   Prompt: 819 + Generación: 50 = 869 tokens total\n",
      "4-SHOT: EXCEDE LÍMITE DE 512 TOKENS\n",
      "   Prompt: 999 + Generación: 50 = 1049 tokens total\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# EJERCICIO 2: FEW SHOT INFERENCING\nprint(\"=\"*80)\nprint(\"EJERCICIO 2: EXPERIMENTACIÓN CON FEW SHOT\")\nprint(\"=\"*80)\n\n# CAMBIO DE ÍNDICES EN example_indices_full y example_index_to_summarize\nprint(\"1. DIFERENTES COMBINACIONES DE DIÁLOGOS\")\nprint(dash_line)\n\n# Configuraciones a probar\ntest_configs = [\n    {\"indices\": [40], \"target\": 200},\n    {\"indices\": [80], \"target\": 200}, \n    {\"indices\": [120], \"target\": 200},\n    {\"indices\": [40], \"target\": 180},\n    {\"indices\": [80], \"target\": 180},\n]\n\nfor i, config in enumerate(test_configs):\n    example_indices_full = config[\"indices\"]\n    example_index_to_summarize = config[\"target\"]\n    \n    prompt = make_prompt(example_indices_full, example_index_to_summarize)\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0],\n        skip_special_tokens=True\n    )\n    \n    target_summary = dataset['test'][example_index_to_summarize]['summary']\n    print(f\"CONFIG {i+1} - Ejemplos: {example_indices_full}, Target: {example_index_to_summarize}\")\n    print(f\"REAL: {target_summary}\")\n    print(f\"GENERADO: {output}\\n\")\n\n# CAMBIO DEL NÚMERO DE EJEMPLOS (SHOTS)\nprint(dash_line)\nprint(\"2. VARIACIÓN EN NÚMERO DE SHOTS\")\nprint(dash_line)\n\nshot_configs = [\n    [40],           # 1-shot\n    [40, 80],       # 2-shot  \n    [40, 80, 120],  # 3-shot\n    [40, 80, 120, 150] # 4-shot\n]\n\ntarget = 200\ntarget_summary = dataset['test'][target]['summary']\nprint(f\"TARGET: {target} - RESUMEN REAL: {target_summary}\\n\")\n\nfor j, indices in enumerate(shot_configs):\n    # Verifico longitud del prompt antes de generar\n    test_prompt = make_prompt(indices, target)\n    token_count = len(tokenizer.encode(test_prompt))\n    \n    # Respeto el límite de 512 tokens: prompt + max_new_tokens(50) debe ser < 512\n    if token_count < 462:  # 512 - 50 = 462 tokens máximo para el prompt\n        inputs = tokenizer(test_prompt, return_tensors='pt')\n        output = tokenizer.decode(\n            model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0],\n            skip_special_tokens=True\n        )\n        print(f\"{len(indices)}-SHOT (tokens: {token_count}): {output}\")\n    else:\n        total_tokens = token_count + 50\n        print(f\"{len(indices)}-SHOT: EXCEDE LÍMITE DE 512 TOKENS\")\n        print(f\"   Prompt: {token_count} + Generación: 50 = {total_tokens} tokens total\")\n\nprint(f\"\\n{dash_line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - ¿Qué puedes observar al cambiar el número de ejemplos?\n",
    "        - Solo 1-shot es viable (392 tokens), mientras que 2-shot (631), 3-shot (819) y 4-shot (999 tokens) exceden \n",
    "        el límite de 512. Esto demuestra que FLAN-T5-base tiene severas limitaciones para few-shot learning con \n",
    "        diálogos largos.\n",
    "\n",
    "    - ¿Cómo de bien funciona few shot inferencing para otros ejemplos?\n",
    "        - Los resultados muestran calidad inconsistente: Target 200 genera resúmenes detallados pero con \n",
    "        alucinaciones (\"CD-ROM drive\", \"painting program\"), mientras Target 180 produce resúmenes extremadamente \n",
    "        simplificados (\"Jack's classes were not bad\") que pierden información clave. El ejemplo usado ([40] vs [80] vs [120]) \n",
    "        no mejora significativamente la precisión.\n",
    "\n",
    "    - Observación clave: Few-shot (1-shot) invierte roles sistemáticamente - convierte \"Person1 teaches Person2\" en \"Person1 \n",
    "    wants to upgrade\", manteniendo errores conceptuales independientemente de la configuración utilizada.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPnXLqEpENNc"
   },
   "source": [
    "## 5 - Configuración de los parámetros de inferencia para la generación\n",
    "\n",
    "Puedes cambiar la configuración de parámetros del metodo `generate()` para modificar la salida del LLM. De momento, el único parámetro que hemos fijado ha sido `max_new_tokens=50`, lo que define el máximo número de tokens a generar. La lista completa de parámetros disponible puede encontrarse aquí: [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).\n",
    "\n",
    "Una forma de organizar los parámetros de configuración es usar la clase `GenerationConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9ez-AkHESd3"
   },
   "source": [
    "**Ejercicio:**\n\nCambia la configuración de parámetros para investigar su influencia en el funcionamiento del modelo LLM.\n\nSi ponemos el parámetro `do_sample = True` activaremos varias formas de decodificación que van a influir en la selección del siguiente token a partir de la probabilidad de distribución. A continuación puedes ajustar las salidas cambiando parámetros como `temperature`, `top_k` y `top_p`.\n\nEn la celda inferior tienes algunas ideas que probar, además se recomienda ponerse creativo y juguetear un poco con otras opciones, incluso con algunas un poco extremas!\n\nQué pasa si ponemos max_new_tokens a 10\nQué conseguimos mediante el cambio de temperature cuando do_sample = True\nQué otras cosas observas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "oYQQyoLTEUr-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPUESTA DIRECTA: max_new_tokens=10\n",
      "============================================================\n",
      "max_new_tokens=10: #Person1 wants to upgrade his system.\n",
      "Comparado con baseline: Jack tells #Person1# that business communication is his favorite last year and #Person1# will check it.\n",
      "OBSERVACIÓN: Mucho más conciso y elimina alucinaciones\n",
      "\n",
      "EXPERIMENTOS CON top_k y top_p:\n",
      "============================================================\n",
      "TOP_K VARIATIONS:\n",
      "top_k=5: People are going to upgrade their computers and make their software more powerful and faster.ones want to upgrade their systems as well.ones want to add a painting program to their software.ones wants to add a CD\n",
      "\n",
      "top_k=10: #Person1 recommends upgrading the software to make it more powerful.nsia has a CD-ROM drive and a faster processor. #Person1 wants to use a software on Cds.n\n",
      "\n",
      "top_k=50: Person1 has mentioned adding a painting program to his computer software.a also considers his need to upgrade his hardware.a offers a CD-ROM drive for upgrading the PC and system.a is prepared to help him select\n",
      "\n",
      "TOP_P VARIATIONS:\n",
      "top_p=0.8: The new software is coming out on CDs.ro says they need to upgrade the hardware and the computer.ro asks for the CD-ROM drive.ro agrees.ro also needs a CD\n",
      "\n",
      "top_p=0.9: #Person1 wants to upgrade the hardware on her computer, but has no ideas of what's needed.ra may suggest adding a painting program to her software.ra suggests a CD-ROM drive, because most\n",
      "\n",
      "top_p=0.95: The software's hardware is in outdated so it is worthwhile updating it.rad may add a painting program to it, but he's not sure about the value of the hardware.rad suggests to add CD-ROM drive and\n",
      "\n",
      "RESPUESTA DIRECTA: Comparación de temperaturas\n",
      "============================================================\n",
      "temperature=0.1: #Person1 recommends upgrading their system.snel is not sure what to do.snel is considering adding\n",
      "temperature=1.0: #Person1 wants to upgrade his software, but finds it difficult to make much more money.te considers adding a painting program\n",
      "temperature=2.0: Adding the option of art to the software would save a significant amount of time on people of computer. Also, one has to upgrade an existing\n",
      "OBSERVACIÓN: A mayor temperature, mayor creatividad pero menor coherencia\n",
      "\n",
      "CONFIGURACIONES EXTREMAS:\n",
      "============================================================\n",
      "Ultra restrictivo (top_k=1, temp=0.01):\n",
      "RESULTADO: #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.ta is not available.ta is not a CD\n",
      "\n",
      "Ultra creativo (temp=2.0, top_p=0.99):\n",
      "RESULTADO: #One is the new CEO so a \"buy something now\" enables a \"done now...\" model new Windows software to be created through computer computers in the early 21 day period because the people, software engineers (in the previous decade\n",
      "\n",
      "Muy corto + caótico (5 tokens, temp=3.0):\n",
      "RESULTADO: According this information. According\n",
      "\n",
      "Híbrido restrictivo (top_k=3, top_p=0.5):\n",
      "RESULTADO: #Person1 recommends adding a painting program to the software. #Person2 recommends adding a CD-ROM drive.one1 recommends adding a CD-ROM drive.one2 recommends adding a\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ¿Qué pasa si ponemos max_new_tokens a 10?\n",
    "print(\"RESPUESTA DIRECTA: max_new_tokens=10\")\n",
    "print(\"=\"*60)\n",
    "generation_config_10 = GenerationConfig(\n",
    "    max_new_tokens=10,\n",
    "    decoder_start_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output_10 = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], generation_config=generation_config_10)[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(f\"max_new_tokens=10: {output_10}\")\n",
    "print(f\"Comparado con baseline: {summary}\")\n",
    "print(\"OBSERVACIÓN: Mucho más conciso y elimina alucinaciones\\n\")\n",
    "\n",
    "print(\"EXPERIMENTOS CON top_k y top_p:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Experimento con top_k\n",
    "print(\"TOP_K VARIATIONS:\")\n",
    "top_k_configs = [5, 10, 50]\n",
    "for k in top_k_configs:\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=k,\n",
    "        decoder_start_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], generation_config=generation_config)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"top_k={k}: {output}\\n\")\n",
    "\n",
    "# Experimento con top_p\n",
    "print(\"TOP_P VARIATIONS:\")\n",
    "top_p_configs = [0.8, 0.9, 0.95]\n",
    "for p in top_p_configs:\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=p,\n",
    "        decoder_start_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], generation_config=generation_config)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"top_p={p}: {output}\\n\")\n",
    "\n",
    "# ¿Qué conseguimos mediante el cambio de temperature?\n",
    "print(\"RESPUESTA DIRECTA: Comparación de temperaturas\")\n",
    "print(\"=\"*60)\n",
    "temp_comparison = [0.1, 1.0, 2.0]\n",
    "for temp in temp_comparison:\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        decoder_start_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], generation_config=generation_config)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"temperature={temp}: {output}\")\n",
    "print(\"OBSERVACIÓN: A mayor temperature, mayor creatividad pero menor coherencia\\n\")\n",
    "\n",
    "# CONFIGURACIONES EXTREMAS\n",
    "print(\"CONFIGURACIONES EXTREMAS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "extreme_configs = [\n",
    "    {\n",
    "        \"name\": \"Ultra restrictivo (top_k=1, temp=0.01)\",\n",
    "        \"config\": GenerationConfig(\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.01,\n",
    "            top_k=1,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ultra creativo (temp=2.0, top_p=0.99)\",\n",
    "        \"config\": GenerationConfig(\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=2.0,\n",
    "            top_p=0.99,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Muy corto + caótico (5 tokens, temp=3.0)\",\n",
    "        \"config\": GenerationConfig(\n",
    "            max_new_tokens=5,\n",
    "            do_sample=True,\n",
    "            temperature=3.0,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Híbrido restrictivo (top_k=3, top_p=0.5)\",\n",
    "        \"config\": GenerationConfig(\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=1.0,\n",
    "            top_k=3,\n",
    "            top_p=0.5,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "for experiment in extreme_configs:\n",
    "    inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(inputs[\"input_ids\"], generation_config=experiment[\"config\"])[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"{experiment['name']}:\")\n",
    "    print(f\"RESULTADO: {output}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - ¿Qué pasa si ponemos max_new_tokens a 10?\n",
    "        - Resultado: \"#Person1 wants to upgrade his system\" - Produce el resumen más limpio y preciso, eliminando \n",
    "        completamente las alucinaciones sobre CD-ROM y painting programs que aparecen en versiones más largas.\n",
    "\n",
    "    - ¿Qué conseguimos mediante el cambio de temperature cuando do_sample = True?\n",
    "        - Temperature 0.1: Texto estructurado pero repetitivo. Temperature 1.0: Introduce errores \n",
    "        lógicos (\"turn in computer software\"). Temperature 2.0: Genera texto casi incoherente sobre \"SD cards\" y \n",
    "        aplicaciones inexistentes.\n",
    "\n",
    "    - ¿Qué otras cosas observas?\n",
    "        - Los parámetros top_k y top_p producen degradación progresiva - fragmentos como \".te is unsure\", \".raeces.com\", \n",
    "        \".snel1\" indican que el modelo genera tokens corruptos. Las configuraciones extremas revelan colapso total: \"temp=3.0\" \n",
    "        produce solo \"#Person#:\" y configuraciones ultra restrictivas generan loops infinitos (\".ta is not available.ta is \n",
    "        not a CD\").\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}