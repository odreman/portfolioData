{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDJPenDPRT8a"
   },
   "source": [
    "# Preprocesamiento de Texto para Clasificación Multi-Etiqueta de StackOverflow\n",
    "\n",
    "Este proyecto tiene como objetivo preprocesar texto y construir features para clasificación multi-etiqueta de posts de StackOverflow. Debido a la longitud del proyecto, está dividido en dos partes. En esta primera parte nos enfocaremos en el preprocesado y construcción de features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUVt_FmKRT8c"
   },
   "source": [
    "Este proyecto tiene como objetivo predecir etiquetas de *posts* de [StackOverflow](https://stackoverflow.com). Técnicamente, es una tarea de clasificación multi-etiqueta. Nótese que el lenguaje en el que están escritas las entradas es el **INGLÉS**, con lo que algunos de los pasos son específicos para dicho idioma.\n",
    "\n",
    "Debido a la longitud del proyecto, está dividido en dos partes. En esta primera parte nos enfocaremos en el preprocesado y construcción de features. Para terminar entrenando modelos en una segunda parte.\n",
    "\n",
    "## Librerías\n",
    "\n",
    "Haremos uso de las siguientes librerías\n",
    "- [Numpy](http://www.numpy.org)\n",
    "- [Pandas](https://pandas.pydata.org)\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html)\n",
    "- [NLTK](http://www.nltk.org) — librería básica para trabajar con texto en Python\n",
    "\n",
    "aunque si quieres pudes usar spaCy para algunas tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ssBGiXgRT8c"
   },
   "source": [
    "##  Preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yzWJnZJRT8d"
   },
   "source": [
    "Una de las primeras técnicas que vamos a utilizar para preprocesar textos es la eliminación de las conocidas como **stop words**, es decir, palabras que no aportan mucho significado, pero que son necesarias para que el texto sea legible y siga las normas. Para ello, lo primero es conseguir una lista con las *stop words* del lenguaje requerido.\n",
    "\n",
    "Una opción para conseguir esta lista de palabras, es usar la librería `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:51.944715Z",
     "start_time": "2021-09-08T11:22:39.059712Z"
    },
    "id": "AWZl5tE8RT8d",
    "outputId": "515a2b6f-cca3-4689-c371-cdd40861dfd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/odremanferrer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeEL8StWRT8f"
   },
   "source": [
    "En este proyecto tenemos un dataset con títulos de entradas de StackOverflow, debidamente etiquetado (con 100 etiquetas distintas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:53.636446Z",
     "start_time": "2021-09-08T11:22:51.946850Z"
    },
    "id": "sVijpWp6RT8f"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import ascii_lowercase\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:53.641835Z",
     "start_time": "2021-09-08T11:22:53.638697Z"
    },
    "id": "im2cEK8kRT8f"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:54.836838Z",
     "start_time": "2021-09-08T11:22:53.644445Z"
    },
    "id": "OwWjj9eKRT8f"
   },
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "train, validation = train_test_split(train, test_size = .15, random_state = 0)\n",
    "test = read_data('data/test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:54.852926Z",
     "start_time": "2021-09-08T11:22:54.838481Z"
    },
    "id": "bqEox5QbRT8g",
    "outputId": "56008f5f-6d85-4ec3-e8b2-2dac287852e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96598</th>\n",
       "      <td>How to create an array of leaf nodes of an htm...</td>\n",
       "      <td>[javascript, arrays, dom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>how to make maven use test resources</td>\n",
       "      <td>[java, maven]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15802</th>\n",
       "      <td>How do I get the path where the user installed...</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9114</th>\n",
       "      <td>why are my buttons not showing up?</td>\n",
       "      <td>[java, swing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>How to loop an array with strings as indexes i...</td>\n",
       "      <td>[php, arrays, string, loops]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "96598  How to create an array of leaf nodes of an htm...   \n",
       "10007               how to make maven use test resources   \n",
       "15802  How do I get the path where the user installed...   \n",
       "9114                  why are my buttons not showing up?   \n",
       "34247  How to loop an array with strings as indexes i...   \n",
       "\n",
       "                               tags  \n",
       "96598     [javascript, arrays, dom]  \n",
       "10007                 [java, maven]  \n",
       "15802                        [java]  \n",
       "9114                  [java, swing]  \n",
       "34247  [php, arrays, string, loops]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16OpnSmXRT8g"
   },
   "source": [
    "Como vemos, la columna `\"title\"` contiene los títulos de las entradas, y la columna `\"tags\"` una lista con las etiquetas de cada entrada, que puede ser un número arbitrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqJj4L5JRT8g"
   },
   "source": [
    "Para seguir los convenios, inicializamos `X_train`, `X_val`, `X_test`, `y_train`, `y_val`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:54.858414Z",
     "start_time": "2021-09-08T11:22:54.854928Z"
    },
    "id": "_r9YwkByRT8g"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test, y_test = test['title'].values, test['tags'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6H1bZSJRT8h"
   },
   "source": [
    "La principal dificultad de trabajar con lenguaje natural es que no está estructurado. Si cojemos el texto y creamos tokens simplemente separando por los espacios, tendremos *tokens* como `'3.5?'`, `'do.'`, etc. Para evitar esos problemas, es útil preprocesar el texto.\n",
    "\n",
    "### **Tarea 1 (Preprocesado):**\n",
    "\n",
    "Implementa la función `text_tokenizer()` y `text_prepare()` siguiendo las instrucciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:24:23.903746Z",
     "start_time": "2021-09-08T11:24:23.897290Z"
    },
    "id": "gd9f0bVLRT8h"
   },
   "outputs": [],
   "source": [
    "def text_tokenizer(text :str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Transforma un texto (str) en una lista de palabras/tokens (list).\n",
    "    Es importante usar esta función siempre para ser consistentes.\n",
    "    \"\"\"\n",
    "    ## ESCRIBE AQUÍ TU CÓDIGO\n",
    "    if not text:  # Si text es None o vacío\n",
    "        return []\n",
    "    return text.split()\n",
    "    ##\n",
    "\n",
    "\n",
    "# cargamos estas variables fuera de la función\n",
    "# ya que la creación del set de stopwords es costoso y no queremos\n",
    "# que se repita cada vez que se llame a la función\n",
    "REPLACE_BY_SPACE = \"[/(){}\\[\\]\\|@,;]\"\n",
    "GOOD_CHARS = ascii_lowercase + \"\".join([str(n) for n in range(10)]) + \" #+_\"\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def text_prepare(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesa el texto inicial:\n",
    "    1. eliminando espacios al inicio y final, y convirtiéndolo a minúsculas\n",
    "    2. cambia los caracteres de REPLACE_BY_SPACE por espacios\n",
    "    3. elimina los caracteres que no estén en GOOD_CHARS\n",
    "    4. elimina los tokens que sean STOPWORDS\n",
    "\n",
    "    text: str\n",
    "    return: str\n",
    "    \"\"\"\n",
    "    ## ESCRIBE AQUÍ TU CÓDIGO\n",
    "    if not text:  # Si text es None o vacío\n",
    "        return \"\"\n",
    "        \n",
    "    # 1. Eliminar espacios y convertir a minúsculas\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # 2. Reemplazar caracteres especiales por espacios\n",
    "    text = re.sub(REPLACE_BY_SPACE, ' ', text)\n",
    "    \n",
    "    # 3. Eliminar caracteres que no estén en GOOD_CHARS\n",
    "    text = ''.join([char for char in text if char in GOOD_CHARS])\n",
    "    \n",
    "    # 4. Eliminar stopwords\n",
    "    tokens = text_tokenizer(text)\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "    ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T11:22:54.872176Z",
     "start_time": "2021-09-08T11:22:54.868817Z"
    },
    "id": "UJCG6djkRT8h",
    "outputId": "a5073b76-cd7d-4079-b2fc-8868b1a421fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Tests correctos!\n"
     ]
    }
   ],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"   SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\",\n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Respuesta incorrecta para: '%s'\" % text_prepare(ex)\n",
    "    return '¡Tests correctos!'\n",
    "\n",
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8pI6FK-RT8i"
   },
   "source": [
    "Ahora preprocesamos los textos de todos los conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mKgtZEAJRT8i"
   },
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]\n",
    "X_test = [text_prepare(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Xrx7GKVURT8i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['create array leaf nodes html dom using javascript',\n",
       " 'make maven use test resources',\n",
       " 'get path user installed java application']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AmIgWPDRT8i"
   },
   "source": [
    "### **Tarea 2 (Cuentas de palabras y etiquetas):**\n",
    "\n",
    "Cuénta cuantas veces aparece cada token (palabra) y cada etiqueta en el corpus de entrenamiento. Es decir, crea un diccionario con las cuentas totales de palabras y etiquetas.\n",
    "\n",
    "El resultado deben ser dos diccionarios *tags_counts* y *words_counts* del tipo `{'palabra_o_etiqueta': cuentas}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XIYjRFReRT8i"
   },
   "outputs": [],
   "source": [
    "# Diccionario con todas las etiquetas del corpus de entrenamiento con sus cuentas\n",
    "tags_counts = {}\n",
    "for tags_list in y_train:\n",
    "    for tag in tags_list:\n",
    "        tags_counts[tag] = tags_counts.get(tag, 0) + 1\n",
    "\n",
    "# Diccionario con todas las palabras del corpus de entrenamiento con sus cuentas\n",
    "words_counts = {}\n",
    "for text in X_train:\n",
    "    for word in text_tokenizer(text):\n",
    "        words_counts[word] = words_counts.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsv4mbNRRT8j"
   },
   "source": [
    "Exploramos las más comunes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sOgEu-ygRT8j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c#', 16259), ('javascript', 16219), ('java', 15835)]\n",
      "[('using', 7000), ('php', 4774), ('java', 4683)]\n"
     ]
    }
   ],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(most_common_tags)\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIrYRdjTRT8j"
   },
   "source": [
    "### Transformando el texto a vectores\n",
    "\n",
    "Vamos a construir los vectores asociados a cada frase en dos representaciones distintas, Bag of Words y tf-idf. Dejaremos la segunda para la parte 2 del proyecto.\n",
    "\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "Recuerda que para crear la representación de *bag of words*, convertimos cada frase en un vector que cuenta el número de ocurrencias de cada token. Se siguien los pasos:\n",
    "1. Encuentra los **N** tokens mas comunes del corpus de entrenamiento y se les asigna un índice, este es nuestro **vocabulario**. Creamos un diccionario para convertir de tokens a índices y viceversa.\n",
    "2. Para cada frase en el corpus, creamos un vector de dimensión **N** y lo inicializamos con ceros.\n",
    "3. Iteramos sobre los tokens de cada frase, y si el token está en el diccionario, incrementamos en 1 el índice correspondiente del vector.\n",
    "   \n",
    "**Tarea 3 (BagOfWords):**\n",
    "\n",
    "Contruye la función que transforma un texto en su representación *bag of words*.\n",
    "\n",
    "Implementa la codificación de *bag of words* en la función `my_bag_of_words()` con un tamaño de diccionario de **N=5000**. Para definir el diccionario, sólo podemos usar el conjunto de entrenamiento, sino tendríamos un *data leaking*.\n",
    "\n",
    "Primero, contruimos el vocabulario y los diccionarios correspondientes, así como un `set` con las palabras del diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ibMrk0A6RT8k"
   },
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:DICT_SIZE]\n",
    "\n",
    "# Crear los diccionarios de mapeo\n",
    "WORDS_TO_INDEX = {word: idx for idx, (word, count) in enumerate(most_common_words)}\n",
    "INDEX_TO_WORDS = {idx: word for word, idx in WORDS_TO_INDEX.items()}\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Lw6aOp7eRT8k"
   },
   "outputs": [],
   "source": [
    "def my_bag_of_words(text: str, words_to_index: dict[str, int]) -> np.array:\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    words_to_index: dict, diccionario con los índices del vocabulario\n",
    "\n",
    "    return\n",
    "    result_vector: numpy.array, vector con la representación bag-of-words de `text`\n",
    "    \"\"\"\n",
    "    dict_size = len(words_to_index)\n",
    "    result_vector = np.zeros(dict_size)\n",
    "\n",
    "    tokens = text_tokenizer(text)\n",
    "    \n",
    "    # Contar ocurrencias de cada palabra que esté en el vocabulario\n",
    "    for token in tokens:\n",
    "        if token in words_to_index:\n",
    "            result_vector[words_to_index[token]] += 1\n",
    "    ##\n",
    "\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZBItIJQ_RT8k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Tests correctos!\n"
     ]
    }
   ],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {\"hi\": 0, \"you\": 1, \"me\": 2, \"are\": 3}\n",
    "    examples = [\"hi how are you\", \"hi hi hi you house\"]\n",
    "    answers = [[1, 1, 0, 1], [3, 1, 0, 0]]\n",
    "    for ex, expected in zip(examples, answers):\n",
    "        output = my_bag_of_words(ex, words_to_index)\n",
    "        if (output != np.asarray(expected)).any():\n",
    "            return f\"Respuesta incorrecta: BOW('{ex}') = {output} != {expected}\"\n",
    "    return \"¡Tests correctos!\"\n",
    "\n",
    "\n",
    "print(test_my_bag_of_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJxEjSbKRT8k"
   },
   "source": [
    "Ahora aplicamos la función anterior a todos los datos.\n",
    "\n",
    "La representación *bag of words* devuelve vectores __*sparse*__ (la mayoría de sus entradas son ceros), con lo que conviene usar estructuras de datos especiales para datos *sparse* para ser eficientes.\n",
    "\n",
    "Hay muchos [tipos de representación sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html), y `sklearn` sólo trabaja con la representación [csr matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), que es la que usamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8cOKJeuiRT8k"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tjdqi_oiRT8l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (85000, 5000)\n",
      "X_val shape  (15000, 5000)\n",
      "X_test shape  (30000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack(\n",
    "    [sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX)) for text in X_train]\n",
    ")\n",
    "X_val_mybag = sp_sparse.vstack(\n",
    "    [sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX)) for text in X_val]\n",
    ")\n",
    "X_test_mybag = sp_sparse.vstack(\n",
    "    [sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX)) for text in X_test]\n",
    ")\n",
    "print(\"X_train shape \", X_train_mybag.shape)\n",
    "print(\"X_val shape \", X_val_mybag.shape)\n",
    "print(\"X_test shape \", X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixI_5vtJRT8l"
   },
   "source": [
    "## Preguntas finales\n",
    "\n",
    "* ¿Qué efecto tienen los caracteres de `REPLACE_BY_SPACE` y `GOOD_CHARS` sobre las features generadas? ¿Se te ocurre una mejor elección de los caracteres escogidos?\n",
    "* Como hemos comentado en el worksheet, la representación Bag of Words no tiene en cuenta el orden de los tokens. Pero hay extensiones de Bag of Words que en cierto grado tienen en cuenta el orden de las palabras. ¿Puedes dar una idea de cómo hacer esto? ¿Crees que puede ser relevante para el contexto del problema?\n",
    "* ¿Cómo actuará un modelo entrenado con estas features ante erratas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuestas finales\n",
    "\n",
    "\"\"\"\n",
    "1. REPLACE_BY_SPACE y GOOD_CHARS:\n",
    "- REPLACE_BY_SPACE separa palabras unidas por caracteres especiales (/(){}[]|@,;)\n",
    "- GOOD_CHARS filtra caracteres no deseados, manteniendo solo letras minúsculas, números y #+_\n",
    "- Mejoraría incluyendo '-' y '.' para nombres de lenguajes y versiones\n",
    "\n",
    "2. Extensión Bag of Words:\n",
    "- Usaría n-gramas para capturar secuencias de palabras\n",
    "- El orden es relevante pero no crítico para clasificación de etiquetas\n",
    "- Ejemplo: \"how to use java\" vs \"java how to use\" tendrían diferentes bigramas\n",
    "\n",
    "3. Comportamiento ante erratas:\n",
    "- El modelo actual es sensible a errores ortográficos\n",
    "- Implementaría:\n",
    "  * Corrección ortográfica\n",
    "  * Stemming\n",
    "  * Fuzzy matching\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: textblob in /opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages (0.19.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/odremanferrer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/odremanferrer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk textblob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejoras implementadas optimizadas:\n",
    "\n",
    "# 1. Mejora de caracteres especiales\n",
    "REPLACE_BY_SPACE = \"[/(){}\\\\[\\\\]\\\\|@,;]\"\n",
    "GOOD_CHARS = ascii_lowercase + \"\".join([str(n) for n in range(10)]) + \" #+_-.\"\n",
    "\n",
    "# 2. Función mejorada de preprocesado con stemming\n",
    "def text_prepare_improved(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesa el texto inicial:\n",
    "    1. eliminando espacios al inicio y final, y convirtiéndolo a minúsculas\n",
    "    2. cambia los caracteres de REPLACE_BY_SPACE por espacios\n",
    "    3. elimina los caracteres que no estén en GOOD_CHARS\n",
    "    4. elimina los tokens que sean STOPWORDS\n",
    "    5. aplica stemming a las palabras\n",
    "\n",
    "    text: str\n",
    "    return: str\n",
    "    \"\"\"\n",
    "    if not text:  # Si text es None o vacío\n",
    "        return \"\"\n",
    "        \n",
    "    # 1. Eliminar espacios y convertir a minúsculas\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # 2. Reemplazar caracteres especiales por espacios\n",
    "    text = re.sub(REPLACE_BY_SPACE, ' ', text)\n",
    "    \n",
    "    # 3. Eliminar caracteres que no estén en GOOD_CHARS\n",
    "    text = ''.join([char for char in text if char in GOOD_CHARS])\n",
    "    \n",
    "    # 4. Eliminar stopwords y aplicar stemming\n",
    "    tokens = text_tokenizer(text)\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    \n",
    "    # 5. Aplicar stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 3. Función para crear n-gramas\n",
    "def create_ngrams(text: str, n: int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    Crea n-gramas a partir de un texto tokenizado.\n",
    "    \n",
    "    text: str, texto a procesar\n",
    "    n: int, tamaño de los n-gramas\n",
    "    return: list[str], lista de n-gramas\n",
    "    \"\"\"\n",
    "    tokens = text_tokenizer(text)\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "# 4. Función mejorada de Bag of Words con n-gramas\n",
    "def my_bag_of_words_improved(text: str, words_to_index: dict[str, int], ngram_size: int = 2) -> np.array:\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    words_to_index: dict, diccionario con los índices del vocabulario\n",
    "    ngram_size: int, tamaño de los n-gramas a considerar\n",
    "\n",
    "    return\n",
    "    result_vector: numpy.array, vector con la representación bag-of-words de `text`\n",
    "    \"\"\"\n",
    "    dict_size = len(words_to_index)\n",
    "    result_vector = np.zeros(dict_size)\n",
    "\n",
    "    # Tokenizar el texto\n",
    "    tokens = text_tokenizer(text)\n",
    "    \n",
    "    # Contar ocurrencias de cada palabra que esté en el vocabulario\n",
    "    for token in tokens:\n",
    "        if token in words_to_index:\n",
    "            result_vector[words_to_index[token]] += 1\n",
    "    \n",
    "    # Añadir n-gramas\n",
    "    ngrams = create_ngrams(text, ngram_size)\n",
    "    for ngram in ngrams:\n",
    "        if ngram in words_to_index:\n",
    "            result_vector[words_to_index[ngram]] += 1\n",
    "\n",
    "    return result_vector\n",
    "\n",
    "# Ejemplo de uso:\n",
    "X_train_improved = [text_prepare_improved(x) for x in X_train]\n",
    "X_train_mybag_improved = sp_sparse.vstack(\n",
    "    [sp_sparse.csr_matrix(my_bag_of_words_improved(text, WORDS_TO_INDEX)) for text in X_train_improved]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos de preprocesado mejorado:\n",
      "Original: create array leaf nodes html dom using javascript\n",
      "Mejorado: creat array leaf node html dom use javascript\n",
      "--------------------------------------------------\n",
      "Original: make maven use test resources\n",
      "Mejorado: make maven use test resourc\n",
      "--------------------------------------------------\n",
      "Original: get path user installed java application\n",
      "Mejorado: get path user instal java applic\n",
      "--------------------------------------------------\n",
      "\n",
      "Ejemplos de n-gramas:\n",
      "Texto original: create array leaf nodes html dom using javascript\n",
      "Bigramas: ['create array', 'array leaf', 'leaf nodes', 'nodes html', 'html dom', 'dom using', 'using javascript']\n",
      "Trigramas: ['create array leaf', 'array leaf nodes', 'leaf nodes html', 'nodes html dom', 'html dom using', 'dom using javascript']\n",
      "--------------------------------------------------\n",
      "\n",
      "Ejemplos de stemming:\n",
      "running -> run\n",
      "jumped -> jump\n",
      "better -> better\n",
      "programming -> program\n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos del preprocesado\n",
    "print(\"Ejemplos de preprocesado mejorado:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {X_train[i]}\")\n",
    "    print(f\"Mejorado: {text_prepare_improved(X_train[i])}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Mostrar algunos n-gramas generados\n",
    "print(\"\\nEjemplos de n-gramas:\")\n",
    "sample_text = X_train[0]\n",
    "print(f\"Texto original: {sample_text}\")\n",
    "print(f\"Bigramas: {create_ngrams(sample_text, 2)}\")\n",
    "print(f\"Trigramas: {create_ngrams(sample_text, 3)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Mostrar el efecto del stemming\n",
    "print(\"\\nEjemplos de stemming:\")\n",
    "sample_words = [\"running\", \"jumped\", \"better\", \"programming\"]\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "for word in sample_words:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.10.0"
   }
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
