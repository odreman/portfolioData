{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujXv4bKhV1jM"
   },
   "source": [
    "# Chatbot de Atención al Cliente con Gradio y OpenAI\n",
    "\n",
    "## Descripción\n",
    "Este proyecto implementa un **chatbot de atención al cliente** utilizando la biblioteca **Gradio** para la interfaz de usuario y la **API de OpenAI** para generar respuestas. El chatbot está configurado para una tienda de electrónica (TechWorld) y responde preguntas sobre productos específicos.\n",
    "\n",
    "El chatbot mantiene el **historial de conversación** para proporcionar respuestas más coherentes. Además, incluye un **mensaje de rol \"system\"** con instrucciones específicas para el chatbot siguiendo la **técnica de Chain of Thought**, definiendo con detalle todos los pasos que debe seguir el asistente para generar la contestación.\n",
    "\n",
    "Para ganar familiaridad con el modelo de costes del API de OpenAI, al final de cada respuesta del chatbot aparece un mensaje indicando el coste total de dicha respuesta, teniendo en cuenta los tokens de entrada, los de salida y su precio (ver https://openai.com/api/pricing/).\n",
    "\n",
    "## Requisitos\n",
    "Antes de comenzar, asegúrate de tener una cuenta en OpenAI y una **clave API**. Usa los **secretos de Google Colab** para almacenar la clave API o asume un fichero *dotenv*.\n",
    "\n",
    "## Instalación de Librerías\n",
    "Ejecuta la siguiente celda en Google Colab para instalar las dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28439,
     "status": "ok",
     "timestamp": 1747214031826,
     "user": {
      "displayName": "Enrique Martín Martín",
      "userId": "07057741903983494535"
     },
     "user_tz": -120
    },
    "id": "aeFRFsCEEcEj",
    "outputId": "1382b9a5-98a4-497b-e847-09f0f64b7d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.10.0 (from gradio)\n",
      "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
      "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx4kCff1EcEk"
   },
   "source": [
    "## Implementación del Chatbot\n",
    "A continuación, se presenta un ejemplo básico de implementación del chatbot en Gradio con respuestas aleatorias. Deberás analizar y ampliar el código para conectarlo con un modelo de OpenAI y usar adecuadamente el historial de la conversación en cada invocación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1747214152059,
     "user": {
      "displayName": "Enrique Martín Martín",
      "userId": "07057741903983494535"
     },
     "user_tz": -120
    },
    "id": "Hfyfc5BPEcEk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# Cargo mi clave API desde un archivo .env\n",
    "#load_dotenv()\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "\n",
    "# Configuro el modelo y precios para calcular costes\n",
    "MODEL = \"gpt-3.5-turbo\"  # Cambio a un modelo que soporte el rol \"system\"\n",
    "INPUT_PRICE = 0.0015  # $ por 1M tokens\n",
    "OUTPUT_PRICE = 0.002  # $ por 1M tokens\n",
    "\n",
    "# Función para calcular el precio de cada consulta\n",
    "def calcular_precio(response):\n",
    "    response_data = response.model_dump()\n",
    "    completion_tokens = response_data[\"usage\"][\"completion_tokens\"]\n",
    "    prompt_tokens = response_data[\"usage\"][\"prompt_tokens\"]\n",
    "\n",
    "    precio_total = prompt_tokens * INPUT_PRICE * 1e-6 + completion_tokens * OUTPUT_PRICE * 1e-6\n",
    "\n",
    "    return f\"\\n\\n_Coste de esta consulta: ${precio_total:.6f}_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747214162490,
     "user": {
      "displayName": "Enrique Martín Martín",
      "userId": "07057741903983494535"
     },
     "user_tz": -120
    },
    "id": "HnjXHZy3EcEl"
   },
   "outputs": [],
   "source": [
    "# Mensaje para el rol \"system\" con Chain of Thought ultra restrictivo\n",
    "developer_message = f\"\"\"\n",
    "Eres un asistente virtual ALTAMENTE RESTRICTIVO para la tienda de electrónica TechWorld.\n",
    "\n",
    "RESTRICCIÓN PRIMARIA: SOLO puedes proporcionar información sobre estos 5 productos específicos:\n",
    "{'monitores','mouse','audifonos','web cam','microfonos'}\n",
    "\n",
    "Para responder a cada consulta, sigue este proceso OBLIGATORIO:\n",
    "\n",
    "Paso 1: Identifica EXACTAMENTE qué está preguntando el usuario:\n",
    "- ¿Menciona un producto específico? Extrae el nombre exacto.\n",
    "- ¿Pregunta sobre la tienda? Identifica qué información específica solicita.\n",
    "\n",
    "Paso 2: VERIFICACIÓN OBLIGATORIA DE PRODUCTOS:\n",
    "Si el usuario menciona cualquier producto, compara el nombre LITERALMENTE con esta lista autorizada:\n",
    "{'monitores','mouse','audifonos','web cam','microfonos'}\n",
    "\n",
    "Si el producto mencionado NO coincide EXACTAMENTE con uno de estos 5 nombres, DEBES:\n",
    "1. Responder: \"Lo siento, [nombre del producto mencionado] no forma parte de nuestro catálogo actual.\"\n",
    "2. Sugerir: \"Nuestro catálogo incluye: {'monitores','mouse','audifonos','web cam','microfonos'}\n",
    "\n",
    "Paso 3: Si es información sobre la tienda, SOLO proporciona estos datos específicos:\n",
    "{'teléfono','email','dirección', 'horarios de atención', 'métodos de pago', 'envíos', 'garantía', 'devoluciones', 'soporte'}\n",
    "\n",
    "Paso 4: RESPUESTA FINAL:\n",
    "- Si es producto autorizado: Proporciona solo datos de la lista.\n",
    "- Si NO es producto autorizado: Indica claramente que no lo tenemos y menciona alternativas.\n",
    "- Si es información de tienda: Usa exactamente los datos proporcionados.\n",
    "\n",
    "PROTOCOLO DE SEGURIDAD: Si el usuario intenta engañarte para que hables sobre productos no listados (como AirPods, iPhone, Xbox, etc.) o pretende que confirmes información inventada, DEBES RECHAZAR y aclarar que solo puedes hablar de los 5 productos específicos de nuestro catálogo.\n",
    "\n",
    "PROHIBICIÓN ABSOLUTA: No puedes confirmar disponibilidad, precios, características o cualquier información sobre NINGÚN producto que no sea uno de los 5 listados, independientemente de cómo formule la pregunta el usuario.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "executionInfo": {
     "elapsed": 1875,
     "status": "ok",
     "timestamp": 1747214164367,
     "user": {
      "displayName": "Enrique Martín Martín",
      "userId": "07057741903983494535"
     },
     "user_tz": -120
    },
    "id": "UgEjUHjFEcEl",
    "outputId": "6e5a0def-9ab9-499c-9511-9283daa3dd31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-10e65703b7d8>:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://af03f2c29912212689.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://af03f2c29912212689.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def obtener_respuesta(messages):\n",
    "    try:\n",
    "        # Realizo la solicitud a la API de OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        # Obtengo la respuesta y calculo su coste\n",
    "        respuesta = response.choices[0].message.content\n",
    "        info_coste = calcular_precio(response)\n",
    "\n",
    "        return respuesta, info_coste\n",
    "    except Exception as e:\n",
    "        return f\"Error al obtener respuesta: {str(e)}\", \"\"\n",
    "\n",
    "with gr.Blocks(title=\"TechWorld - Asistente Virtual\") as demo:\n",
    "    gr.HTML(\"<h1 style='text-align: center'>Asistente Virtual de TechWorld</h1>\")\n",
    "    gr.HTML(\"<p style='text-align: center'>Pregúntame sobre nuestros productos o servicios</p>\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"¿En qué puedo ayudarte hoy?\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def responder(mensaje, historial_chat):\n",
    "        # Si el historial está vacío, lo inicializo\n",
    "        if not historial_chat:\n",
    "            historial_chat = []\n",
    "\n",
    "        # Añado el mensaje del usuario al historial\n",
    "        historial_chat.append([mensaje, None])\n",
    "\n",
    "        # Preparo mensajes para la API con el mensaje del desarrollador como system\n",
    "        mensajes_api = [\n",
    "            {\"role\": \"system\", \"content\": developer_message}\n",
    "        ]\n",
    "\n",
    "        # Convierto el historial de Gradio al formato de OpenAI\n",
    "        for mensaje_usuario, mensaje_asistente in historial_chat:\n",
    "            if mensaje_usuario:\n",
    "                mensajes_api.append({\"role\": \"user\", \"content\": mensaje_usuario})\n",
    "            if mensaje_asistente:\n",
    "                # Excluyo la parte del coste para futuras interacciones\n",
    "                contenido_sin_coste = mensaje_asistente.split(\"\\n\\n_Coste de esta consulta\")[0]\n",
    "                mensajes_api.append({\"role\": \"assistant\", \"content\": contenido_sin_coste})\n",
    "\n",
    "        # Obtengo la respuesta y su coste\n",
    "        respuesta, info_coste = obtener_respuesta(mensajes_api)\n",
    "\n",
    "        # Actualizo el historial con la respuesta y su coste\n",
    "        historial_chat[-1][1] = respuesta + info_coste\n",
    "\n",
    "        return \"\", historial_chat\n",
    "\n",
    "    msg.submit(responder, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZs7dP0NEcEm"
   },
   "source": [
    "## Preguntas de Reflexión\n",
    "1. ¿Cómo afecta el historial de conversación a la calidad de las respuestas del chatbot?\n",
    "2. ¿Qué cambios harías para mejorar la experiencia del usuario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1747214164446,
     "user": {
      "displayName": "Enrique Martín Martín",
      "userId": "07057741903983494535"
     },
     "user_tz": -120
    },
    "id": "QTxfEIvIEcEm",
    "outputId": "03e6749b-5224-4884-90db-feaca6bc27b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n1. ¿Cómo afecta el historial de conversación a la calidad de las respuestas del chatbot?\\nEn mi implementación inicial, el chatbot tenía \"amnesia\" al no enviar conversaciones previas al modelo, lo que provocaba \\nrespuestas inconsistentes y obligaba al usuario a repetir información. Un buen historial permite mantener contexto, personalizar \\nrespuestas y resolver consultas que requieren múltiples intercambios.\\n\\n\\n\\n2. ¿Qué cambios harías para mejorar la experiencia del usuario?\\nMejoré la función responder() para enviar el historial completo en cada interacción con la API, permitiendo que el \\nchatbot \"recuerde\" toda la conversación anterior. También limpié los metadatos de costes antes de reenviar las \\nrespuestas, garantizando consistencia en diálogos extensos.\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "1. ¿Cómo afecta el historial de conversación a la calidad de las respuestas del chatbot?\n",
    "En mi implementación inicial, el chatbot tenía \"amnesia\" al no enviar conversaciones previas al modelo, lo que provocaba\n",
    "respuestas inconsistentes y obligaba al usuario a repetir información. Un buen historial permite mantener contexto, personalizar\n",
    "respuestas y resolver consultas que requieren múltiples intercambios.\n",
    "\n",
    "\n",
    "\n",
    "2. ¿Qué cambios harías para mejorar la experiencia del usuario?\n",
    "Mejoré la función responder() para enviar el historial completo en cada interacción con la API, permitiendo que el\n",
    "chatbot \"recuerde\" toda la conversación anterior. También limpié los metadatos de costes antes de reenviar las\n",
    "respuestas, garantizando consistencia en diálogos extensos.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFGoSsq5EcEn"
   },
   "source": [
    "Version 2 con mejora del historial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "executionInfo": {
     "elapsed": 2344,
     "status": "ok",
     "timestamp": 1747214185113,
     "user": {
      "displayName": "Enrique Martín Martín",
      "userId": "07057741903983494535"
     },
     "user_tz": -120
    },
    "id": "hYOMfvk-EcEn",
    "outputId": "6af2ddea-6288-4d39-98f9-7ed9388b7d62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0326e22c3150>:128: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://3f20c2c8ae976cbf72.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3f20c2c8ae976cbf72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "import time\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "# Cargar variables de entorno\n",
    "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Configurar cliente de OpenAI\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Definir el modelo a utilizar\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Catálogo de productos\n",
    "productos = \"\"\"\n",
    "Categoría: Smartphones\n",
    "- Samsung Galaxy S23, 999€, Disponible en: Negro, Blanco, Verde\n",
    "- Xiaomi 13, 799€, Disponible en: Negro, Azul\n",
    "- iPhone 14, 1099€, Disponible en: Negro, Blanco, Rojo\n",
    "\n",
    "Categoría: Portátiles\n",
    "- Lenovo ThinkPad X1, 1499€, Características: Intel i7, 16GB RAM, 512GB SSD\n",
    "- MacBook Air M2, 1299€, Características: Apple M2, 8GB RAM, 256GB SSD\n",
    "- HP Spectre x360, 1399€, Características: Intel i7, 16GB RAM, 1TB SSD\n",
    "\n",
    "Categoría: Tablets\n",
    "- iPad 10ª gen, 499€, 64GB, WiFi\n",
    "- Samsung Galaxy Tab S8, 749€, 128GB, WiFi+5G\n",
    "- Lenovo Tab P11, 329€, 128GB, WiFi\n",
    "\n",
    "Categoría: Accesorios\n",
    "- Auriculares Sony WH-1000XM5, 399€\n",
    "- Ratón Logitech MX Master 3, 99€\n",
    "- Monitor LG UltraGear 27\", 349€\n",
    "\"\"\"\n",
    "\n",
    "# Información de la tienda\n",
    "informacion_tienda = \"\"\"\n",
    "Horarios: Lunes a Viernes de 10:00 a 20:00, Sábados de 10:00 a 14:00\n",
    "Ubicaciones:\n",
    "- Madrid: Calle Gran Vía 41\n",
    "- Barcelona: Passeig de Gràcia 92\n",
    "- Valencia: Calle Colón 15\n",
    "Políticas:\n",
    "- Envíos: Gratuitos para compras superiores a 50€. Entrega en 24-48h.\n",
    "- Devoluciones: 30 días para devolver productos en perfecto estado.\n",
    "- Garantía: 2 años en todos los productos electrónicos.\n",
    "\"\"\"\n",
    "\n",
    "# Mensaje para el rol \"system\" con Chain of Thought mejorado\n",
    "developer_message = f\"\"\"\n",
    "Eres un asistente virtual para la tienda de electrónica TechWorld.\n",
    "\n",
    "Para responder a cada consulta, sigue ESTRICTAMENTE estos pasos:\n",
    "\n",
    "Paso 1: Analiza la consulta para determinar si pregunta sobre:\n",
    "- Producto específico o categoría\n",
    "- Información de la tienda (horarios, ubicación)\n",
    "- Políticas (envíos, devoluciones, garantía)\n",
    "- Otro tipo de información\n",
    "\n",
    "Paso 2: Si es sobre un producto, verifica RIGUROSAMENTE si está en nuestra lista oficial:\n",
    "{productos}\n",
    "IMPORTANTE: Si el producto NO está en esta lista exacta, NO debes responder como si lo tuviéramos. Informa amablemente que no disponemos de ese producto en nuestro catálogo actual.\n",
    "\n",
    "Paso 3: Si es sobre información de la tienda, identifica qué información específica necesita de esta lista:\n",
    "{informacion_tienda}\n",
    "\n",
    "Paso 4: Determina qué información precisa debes proporcionar basándote ÚNICAMENTE en los datos anteriores.\n",
    "\n",
    "Paso 5: Formula una respuesta amigable y profesional que:\n",
    "- Responda directamente a la consulta cuando tengamos la información\n",
    "- Claramente indique cuando no tengamos el producto solicitado\n",
    "- Ofrezca alternativas de productos similares de nuestro catálogo cuando sea apropiado\n",
    "\n",
    "REGLA CRÍTICA: NUNCA inventes información, características, precios o disponibilidad de productos que no estén explícitamente listados arriba. Si un cliente menciona un producto que no está en nuestra lista (como AirPods, PlayStation, etc.), debes indicar claramente que no lo tenemos en nuestro catálogo actual y sugerir alternativas de nuestra lista si son relevantes.\n",
    "\n",
    "Si no estás seguro de algo, indica que no tienes esa información específica y ofrece ayuda alternativa.\n",
    "\"\"\"\n",
    "\n",
    "# Función para calcular el coste de la respuesta\n",
    "def calcular_precio(response):\n",
    "    # Obtener tokens de entrada y salida\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "\n",
    "    # Precios por 1000 tokens (según la documentación de OpenAI)\n",
    "    prompt_price_per_1k = 0.0015  # $0.0015 por 1000 tokens para gpt-3.5-turbo\n",
    "    completion_price_per_1k = 0.002  # $0.002 por 1000 tokens para gpt-3.5-turbo\n",
    "\n",
    "    # Calcular costos\n",
    "    prompt_cost = prompt_tokens * (prompt_price_per_1k / 1000)\n",
    "    completion_cost = completion_tokens * (completion_price_per_1k / 1000)\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "\n",
    "    # Formatear la información\n",
    "    info = f\"\\n\\n_Coste de esta consulta: ${total_cost:.6f} | Tokens: {prompt_tokens} prompt, {completion_tokens} completion_\"\n",
    "\n",
    "    return info\n",
    "\n",
    "# Función para obtener respuesta de OpenAI\n",
    "def obtener_respuesta(messages):\n",
    "    try:\n",
    "        # Realizar la solicitud a la API de OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        # Obtener la respuesta del asistente\n",
    "        respuesta = response.choices[0].message.content\n",
    "\n",
    "        # Calcular y añadir información sobre el coste\n",
    "        info_coste = calcular_precio(response)\n",
    "\n",
    "        return respuesta, info_coste\n",
    "    except Exception as e:\n",
    "        return f\"Error al obtener respuesta: {str(e)}\", \"\"\n",
    "\n",
    "# Crear la interfaz con Gradio\n",
    "with gr.Blocks(title=\"TechWorld - Asistente Virtual\") as demo:\n",
    "    gr.HTML(\"<h1 style='text-align: center; margin-bottom: 1rem'>Asistente Virtual de TechWorld</h1>\")\n",
    "    gr.HTML(\"<p style='text-align: center'>Bienvenido a TechWorld. Pregúntame sobre nuestros productos, servicios o políticas de la tienda.</p>\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"Escribe tu pregunta aquí y presiona Enter...\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def responder(mensaje, historial_chat):\n",
    "        # Mostrar mensaje de espera\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # Si el historial está vacío, inicializarlo\n",
    "        if not historial_chat:\n",
    "            historial_chat = []\n",
    "\n",
    "        # Añadir mensaje del usuario al historial\n",
    "        historial_chat.append([mensaje, None])\n",
    "\n",
    "        # Preparar mensajes para enviar a la API\n",
    "        mensajes_api = [\n",
    "            {\"role\": \"system\", \"content\": developer_message}\n",
    "        ]\n",
    "\n",
    "        # Convertir TODO el historial de Gradio al formato para OpenAI API\n",
    "        for mensaje_usuario, mensaje_asistente in historial_chat:\n",
    "            if mensaje_usuario:\n",
    "                mensajes_api.append({\"role\": \"user\", \"content\": mensaje_usuario})\n",
    "            if mensaje_asistente:\n",
    "                # No incluir la parte del coste en el historial para la API\n",
    "                contenido_sin_coste = mensaje_asistente.split(\"\\n\\n_Coste de esta consulta\")[0]\n",
    "                mensajes_api.append({\"role\": \"assistant\", \"content\": contenido_sin_coste})\n",
    "\n",
    "        # Obtener respuesta de la API de OpenAI\n",
    "        respuesta, info_coste = obtener_respuesta(mensajes_api)\n",
    "\n",
    "        # Actualizar el último mensaje del asistente (incluye coste)\n",
    "        historial_chat[-1][1] = respuesta + info_coste\n",
    "\n",
    "        return \"\", historial_chat\n",
    "\n",
    "    msg.submit(responder, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "# Iniciar la demo\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
